# Resources for Deep Learning Model for dealing with multiple different languages

Different way to deal with multiple languages for comparison is to convert them into same space of embedding or to translate the multi language into one common language.

<details>
  <summary></summary>
  
  [Link]()
  
</details>

<details>
  <summary>mBART-50</summary>
  
  [Link](https://github.com/sdhilip200/Machine-Translation-using-mBART-50-and-HuggingFace/blob/main/Machine_Translation.ipynb)
  
  [Link](https://huggingface.co/facebook/mbart-large-50-many-to-many-mmt)
  
  mBART-50 is a language translation model. This model supports 50 different languages.
  Language codes are as follows:
  
 mbart = {
'Arabic' : 'ar_AR', 
'Czech' : 'cs_CZ', 
'German' : 'de_DE',
'English': 'en_XX', 
'Spanish' : 'es_XX', 
'Estonian' : 'et_EE', 
'Finnish' : 'fi_FI', 
'French' : 'fr_XX', 
'Gujarati' : 'gu_IN', 
'Hindi': 'hi_IN', 
'Italian': 'it_IT', 
'Japanese' : 'ja_XX', 
'Kazakh' : 'kk_KZ', 
'Korean' : 'ko_KR', 
'Lithuanian' : 'lt_LT', 
'Latvian' : 'lv_LV', 
'Burmese': 'my_MM', 
'Nepali' : 'ne_NP', 
'Dutch' : 'nl_XX', 
'Romanian': 'ro_RO', 
'Russian' : 'ru_RU', 
'Sinhala' : 'si_LK', 
'Turkish' : 'tr_TR', 
'Vietnamese' : 'vi_VN', 
'Chinese': 'zh_CN', 
'Afrikaans' : 'af_ZA', 
'Azerbaijani' : 'az_AZ',
'Bengali' : 'bn_IN', 
'Persian' : 'fa_IR', 
'Hebrew' : 'he_IL', 
'Croatian' : 'hr_HR', 
'Indonesian' : 'id_ID', 
'Georgian': 'ka_GE', 
'Khmer' : 'km_KH',
'Macedonian' : 'mk_MK', 
'Malayalam' : 'ml_IN', 
'Mongolian' : 'mn_MN', 
'Marathi' : 'mr_IN', 
'Polish' : 'pl_PL', 
'Pashto' : 'ps_AF',
'Portuguese' : 'pt_XX', 
'Swedish': 'sv_SE', 
'Swahili' : 'sw_KE', 
'Tamil' : 'ta_IN', 
'Telugu' : 'te_IN',
'Thai' : 'th_TH', 
'Tagalog' : 'tl_XX', 
'Ukrainian' : 'uk_UA', 
'Urdu' : 'ur_PK', 
'Xhosa' : 'xh_ZA',
'Galician' :'gl_ES', 
'Slovene' : 'sl_SI'
}
  
</details>


